# Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models

## Authors
- Raghuraman Mudumbai (raghuraman-mudumbai@uiowa.edu)
- Tyler Bell (tyler-bell@uiowa.edu)

## Abstract
We propose a new asymptotic equipartition property for the perplexity of a large piece of text generated by a language model and present theoretical arguments for this property. Perplexity, defined as an inverse likelihood function, is widely used as a performance metric for training language models. Our main result states that the logarithmic perplexity of any large text produced by a language model must asymptotically converge to the average entropy of its token distributions. This means that language models are constrained to only produce outputs from a "typical set", which we show, is a vanishingly small subset of all possible grammatically correct outputs. We present preliminary experimental results from an open-source language model to support our theoretical claims. This work has possible practical applications for understanding and improving "AI detection" tools and theoretical implications for the uniqueness, predictability, and creative potential of generative models.

## Repository Structure
```plaintext
.
├── topk_text_generator.py    # Script to generate text using an LLM
├── natural_text_evaluator.py # Script to evaluate known text against the LLM
├── json-calculator.py        # Script to analyze JSON files and plot results
├── json/                     # Directory for generated JSON files
├── plots/                    # Directory for plots and visualizations
├── LICENSE                   # License file (MIT)
├── README.md                 # Project overview and instructions
```

## Dependencies
- python (we used 3.10.13)
- pytorch (we used 2.1.2, py3.10_cuda11.8_cudnn8_0)
- transformers (4.36.2)
- cuda (11.8)
- matplotlib

## Installation (ymmv)
- $ conda create --name perplexity-llm python=3.10
- $ conda activate perplexity-llm
- $ conda install numpy matplotlib pytorch torchvision transformers

